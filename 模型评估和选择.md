## 训练误差和泛化误差
- 误差：样本的预测输出与真实输出之间的差异。
    - 训练误差：也叫经验误差，是指在训练集中样本的预测输出与真实输出之间的差异。
    - 测试误差：是指在测试集中样本的预测输出与真实输出之间的差异
    - 泛化误差：在新样本上的误差，通常以测试误差作为泛化误差的近似

在进行模型选择时，我们通常希望选择泛化误差最小的模型，但实际能做的是努力使训练误差最小化。但如果学习器把训练集学得太好了，很可能会把训练集自身的一些局部特性也当做全局特性进行了学习，从而导致学习器的泛化能力下降。

- 过拟合（overfitting）：因为学习器学习能力太强，误把训练集自身的一些局部特性当做了全局特性，而导致学习器泛化能力降低的现象。
- 欠拟合（underfitting）：因为学习器学习能力太弱，连训练集的全局特性都没学好，而导致学习器泛化能力降低的现象。

![](/assets/overfit.png)

过拟合是机器学习面临的关键障碍，它只能被缓解而无法彻底避免。

## 泛化能力的实验评估方法
在实验中，我们通常是以测试误差作为泛化误差的近似来对模型的泛化能力进行评估的。

假设现在我们有一个包含m个样本的数据集$$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$$，既要训练又要测试，该怎么做呢？答案是：通过对数据集进行合理划分从而产生所需要的训练集S和测试集T。常用的方法有留出法和交叉验证法。

划分测试集的准则：
1. 分布一致：尽可能保持测试集和训练集数据分布的一致性，否则评估结果将由于训练/测试集分布的差异而产生较大偏差；
2. 互斥：尽可能保持测试集和训练集互斥，否则就“泄题”了，评估结果会过于“乐观”；

实验评估方法的一般过程：① 划分 → ② 训练-测试 → ③ 评估结果

### 留出法（hold-out）
- 原理：把数据集D划分为两个互斥子集，分别作为训练集S和测试集T，用训练集来训练模型，用测试集来评估模型的泛化误差。单次留出法的评估结果往往不够稳定可靠，通常取多次评估结果的平均值作为最终评估结果。

- 抽样方法：通常采用随机抽样或分层抽样

- 抽样比例：二八分，训练集8，测试集2

### 交叉验证法（cross-validation）
- 原理：先把数据集D划分为k个大小相近的互斥子集，然后每次取其中的k-1个子集的并集作为训练集S，取剩下的那个子集作为测试集T，经过k次训练和测试，取k次评估结果的平均值作为最终的评估结果。

![](/assets/k-fold.png)
- 抽样方式：分层采样

- k折交叉验证（k-fold cross-validation）：通常k值越大评估结果越准确，k最常用的取值是10，称10折交叉验证

- 留一法：当k=m时，称为留一法。训练集和D接近，评估结果最准确，但开销大。

## 泛化能力的性能度量

性能度量（performance measure）:衡量模型泛化能力的评价标准。
### 回归任务
回归任务中最常用的性能指标是“均方误差”（Mean squared Error,MSE）：预测值与真实值之差的平方和的平均值。

对于数据分布D和概率密度函数p(x)，均方误差可表示为：

$$
E(f;D)=\int_{(x,y)\in D}(f(x)-y)^{2}p(x)dx
$$

### 分类任务
#### 错误率和精度
错误率和精度是分类任务中最常用的两种性能度量。对于数据集D，
- 错误率：错误分类的样本数a与样容量m的比值
- 精度：精度=1- 错误率

#### 查准率和查全率
有时相对于预测精度，我们更关心查准率和查全率。对二分类问题，根据样本的真实类别和预测类别的不同组合可以定义出真正例（TP）、假正例（FP）、真负例（TN）、假负例（FN），组合结果的“混淆矩阵”（confusion matrix）如下：

真实\预测|正|负
:---:|:---:|---:
正|TP|FN
负|FP|TN

- 查准率（precision）：预测为正的样本中有多少真实为正。
$$
P = \frac{TP}{TP+FP}
$$

- 查全率（recall）：真实为正的样本中有多少预测为正。
$$
R = \frac{TP}{TP+FN}
$$


#### P-R曲线和F1度量
**分类阈值**：很多学习器会为每个测试样本产生一个预测值，将其与分类阈值比较，大于阈值则为正类，否则为负类。

查准率和查全率是一对矛盾，如果把阈值设置的较高则查准率较高但查全率较低，反之查准率较低但查全率较高。我们可以通过P-R曲线来直观反映这种关系。

绘制P-R曲线：按照预测结果是正例的可能性大小对样本进行排序，然后通过调整阈值逐个将样本作为正例并计算当前的查准率P和查全率R，就得到了P-R曲线。

![](/assets/P-R.png)

- 平衡点（Break-Even Point，BEP）:P=R时的取值,BEP较大的模型较优

- F1度量：P和R的调和平均值，越大性能越好

$$
\frac{1}{F_{1}}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})
$$

#### ROC曲线和AUC
受试者工作特性（Receiver Operating Characteristic, ROC）与P-R类似，只不过ROC是使用“假正例率”（FPR）和“真正例率”（TPR）来作为曲线的横纵坐标。

- TPR：真实为正的样例中有多少预测为正（同查全率）。

$$
TPR = \frac{TP}{TP+FN}
$$
- FPR：真实为假的样例中有多少预测为正

$$
FPR = \frac{FP}{TN+FP}
$$

- ROC曲线: 先将样本按预测值进行排序（同P-R曲线），先将阈值设为最大，则所有样本都被预测为负例，TPR = FPR = 0，然后依次减小阈值并重新计算TPR和FPR，当所有样本都被预测为正例时，TPR = FPR = 1。

![](/assets/roc.png)

- 泛化能力比较：我们期望较高的真正例率和较低的假正例率。从图像上看，若一个学习器的ROC曲线被另一个学习器的ROC曲线包裹，则后者性能更好。如果两者交叉，则一般通过ROC曲线下面积AUC（Area Under ROC Curve）来比较，AUC越大性能越好。

## 偏差-方差分解
一般算法的**期望泛化误差**可分解为偏差、方差和噪声之和：

$$
E(f;D) = bias^{2}(x) + var(x) + \varepsilon ^{2}
$$

- 偏差：度量了期望预测结果与真实结果的偏离程度，即**算法本身**的拟合能力；
- 方差：度量了训练集变动导致的学习性能的变化，即**数据扰动**造成的影响；
- 噪声：当前任务任何学习算法所能达到的期望泛化误差的下界，即问题**本身的难度**；

![](/assets/2-1.png)


结论：
- 泛化性能是由算法的拟合能力、数据的充分性、任务本身的难度共同决定的；给定学习任务，为获取较好的泛化能力，应能充分拟合数据，并使得数据扰动产生的影响最小。
- 偏差-方差窘境：偏差和方差是有冲突的
    - 当训练不足时，学习器的拟合能力不强，数据扰动不足以影响泛化能力，偏差主导了泛化误差，发生欠拟合；
    - 随着训练程度的加深，学习器的拟合能力逐渐加强，数据扰动渐渐能被学习器学到
    - 当训练程度充足后，学习器的拟合能力非常强，数据的轻微扰动都会导致学习器发生显著变化，若数据自身的非全局特性被学习器学到则发生过拟合；

![](/assets/b-v.jpeg)








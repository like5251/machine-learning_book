## 写伪代码
- 写伪代码：LR、梯度下降、最小二乘、KNN、Kmeans
- 手推公式：LR、SVM、XGBOOST
- 其他：LR,SVM,RF,KNN，EM，Adaboost,PageRank，GBDT，Xgboost，HMM，DNN，推荐算法，聚类算法，等等机器学习领域的算法

## 基础知识：
#### 1. 监督学习、非监督学习和半监督区别
- 监督学习：训练样本标记信息已知的机器学习叫监督式学习，一般包括分类和回归问题；
- 无监督学习：训练样本标记信息未知的机器学习叫无监督式学习，常见的有聚类算法；
- 半监督学习：利用少量带标记的样本和大量未标记的样本进行机器学习，从概率角度可以理解为通过研究输入的边缘概率和输出的条件概率间的关系来设计分类器的方法。

#### 2. L0、L1和L2区别
机器学习的目的→偏差方差分解→正则项来平衡→范数→特征选择→稀疏约束→L0→L1→L2

- L0范数：向量中非零元素个数（很难用于优化求解）
- L1范数：向量中各元素绝对值之和

机器学习的目的是找到泛化能力最好的模型，这可以分解为两个部分，一部分是使经验误差最小，另一部分是使得经验误差尽可能接近泛化误差。
第一部分要求损失函数最小，也就是使得模型对数据的拟合程度最高，在数据规模一定时，增加模型复杂度可以增加模型对数据的拟合程度，同时过于复杂的模型也可能使得经验误差和泛化误差的差距变大，导致过拟合的发生，这又要求我们选取合适的模型复杂度，在模型偏差和方差之间进行平衡。

添加正则项的目的就是为了在模型偏差和方差间做出恰当的平衡，在保证拟合程度的同时降低模型复杂度，降低过拟合风险。正则项实际上是一种惩罚机制：若增加模型复杂度所换取的在损失函数上的增益不足抵消因此导致的正则项的增加，那么增加这样的复杂度就是得不偿失的。

范数的三个性质：非负、齐次、三角不等式，因此参数向量的范数天然适合作为正则项。

常见的范数：0范数Lo,p范数Lp，无穷范数，最常用的范数是L1和L2


L0 l1 L2都可以起到平衡模型拟合程度和模型复杂度的作用，能够降低模型过拟合的风险。

- L0范数：从特征选择的角度来看，适当减少特征维度可以降低模型复杂度从而降低过拟合的风险。嵌入式特征选择就是在模型训练过程中对参数施加“稀疏性”约束，即使得参数w尽可能多的分量变为0。最直接方式就是使用L0范数，但是L0范数不连续，难以进行优化求解，因此常使用L1范数来代替。（L1是L0的备胎）。

- L1范数：L1比L2更容易产生稀疏解（L1使得更多的参数变为0，趋向于使用少量特征）

- L2：L2对离群点更敏感，L2倾向于是的参数稠密地接近于零（L2倾向于使用更多的特征，但是每个特征的作用都较小）

#### 3. 生成模型和判别模型区别


#### 4. 算法优缺点及相应的解决方案

#### 5. SVM
svm算法的原理、如何组织训练数据、如何调节惩罚因子、如何防止过拟合、svm的泛化能力、增量学习

#### 6. 决策树

#### 7. 朴素贝叶斯

#### 8. 逻辑回归

#### 9.



















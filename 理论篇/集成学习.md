# 集成学习
> 集成学习（ensemble，"昂森宝"，或者aggregation）：通过组合多个学习器以获取更好的泛化性能。

## 一、集成学习的理论基础
集成学习的两个步骤：
1. 产生一组“弱学习器” ：$$g_{1}(\boldsymbol{x})$$、$$g_{2}(\boldsymbol{x})$$、$$g_{3}(\boldsymbol{x})$$......
2. 按照某种规则将弱分类器组合起来：$$G(\boldsymbol{x})=\sum_{t=1}^{T}\alpha_{t} g_{t}(\boldsymbol{x})$$

### 1. 产生弱分类器的方式
> 弱分类器：泛化性能略优于随机猜测的学习器称为弱学习器。它有两层含义：
1. 弱：虽然从理论上说使用弱分类器集成足以获得好的性能，但在实践中为了减少学习器的数量，人们往往会使用比较强的学习器。
2. 强：所选用的弱分类器必须得比随机猜测要好（如二分类误分率>0.5），这样才能“强强联合”，否则该学习器会成为集成中的“害群之马”，应被过滤掉。

#### 1.1 获取弱分类器的方法
1. 基于不同的模型：$$g_{1}\in H_{1},g_{2}\in H_{2},...,g_{T}\in H_{T}$$，如通过逻辑回归和随机森林获取两个不同模型；
2. 基于的不同超参数：如通过设置随机森林不同的超参数来得到多个随机森林模型；
3. 基于不同的训练数据：如bagging的方法（有放回地均匀采样）、随机森林（有放回地均匀采样）、Adaboost（按照误分率改变样本权重）
4. 基于随机算法：如遗传编程、选取不同的seeds等

#### 1.2 集成的原则
集成原则：要获取好的集成，每个分类器应”好而不同“。

1. 好：每个分类器要有一定”准确性“，至少要好于随机猜测；
2. 不同：应保证分类器的”多样性“，通过上述不同的方式获取到区分度较高的基本分类器；

假设各分类器相互独立，则集成错误率随基本学习器数量的增加而指数级下降：

$$
P(G(x)\neq f(x))\leqslant exp(-\frac{1}{2}T(1-2\varepsilon )^2)
$$

”准确性“和”独立性“的矛盾：当准确性很高以后，要增加准确性就得牺牲掉多样性，如何产生”好而不同“的多个学习器正是集成学习中的圣杯问题。

### 2. 弱分类器集成的方式
假设我们已经获取到了多个基本学习器$$g_{1}(\boldsymbol{x})$$、$$g_{2}(\boldsymbol{x})$$、$$g_{3}(\boldsymbol{x})$$......，通常有以下几种方式来进行集成：

1. select:选取验证误差最小的模型作为最终模型:$$G(x)=g_{*} with t_{*}=argmin(E_{val}(g^-_{t}))$$
2. uniform blending(voting)：取每个模型预测结果的的算术平均作为最终预测结果:$$G(x)=sign(\sum_{t=1}^{T}g_{t}(x))$$
3. Linear blending：取所有模型预测结果的加权平均作为最终预测结果:$$G(x)=sign(\sum_{t=1}^{T} \alpha _{t}\cdot g_{t}(x)) with \alpha _{t}\geq 0$$
4. stacking：根据输入有条件地进行组合$$G(x)=sign(\sum_{t=1}^{T}q_{t}(x)\cdot g_{t}(x) with q_{t}(x))\geqslant 0$$

以上集成方法可以看做是对原始特征进行特征转化，将基本模型的预测值作为新的特征，然后基于这些新特征进行二次训练的过程。特征转化非常powerful但也容易发生过拟合，除了voting之外，一般在训练集上训练基本模型，然后在验证集上做二次训练求出基本模型的集成参数！

#### 2.1 select
- 原理：在所有备选模型中选取验证误差最小的模型作为最终模型:
$$
G(x)=g_{*} with t_{*}=argmin(E_{val}(g^-_{t}))
$$

- 分析：强人领导、容易过拟合，需在验证集上做集成
    - 在第一名中选第一名：每个备选模型都是在其所在假设空间$$H_t$$中选优的结果，select方法就是在这些第一名中再选取第一名，对应假设空间的复杂度为$$d_{vc}(\bigcup_{t=1}^{T}H_{t})$$，非常容易发生过拟合。
    - 在验证集上集成（Val-Set blending）：一般会在训练集上训练多个模型，然后在测试集上选择其中泛化选择最好的模型；
    - 强人领导：select方法依赖于备选模型中是否有一个强分类器；如果是在一群弱学习器中进行select则完全没有意义；


#### 2.2 uniform blending(voting)
- 原理：取每个模型预测结果的算术平均作为最终预测结果:

$$
G(x)=sign(\sum_{t=1}^{T}g_{t}(x))
$$

- 分析：集体智慧+民主投票，降低方差，直接在训练集上集成
    - 对于分类问题：
$$
G(x)=\underset{1\leqslant k\leqslant K}{argmax}  \sum_{t=1}^{T}[g_{t}(x)=k]
$$

    - 对于回归问题：

$$
G(x) = \frac{1}{T}\sum_{t=1}^{T}g_{t}(x)
$$

> 可以从偏差方差分解的角度来分析voting时发生了什么：
> - f(x)表示x对应的真实值/观测值（如果忽略观测噪声,可认为观测值等于真实值y=f(x)）
> - g(x)表示x对应的预测值（在某个训练集D上算法A学到g(x)）
> - $$\overline{g}(x)$$表示在相同容量的不同训练集D上基于给定算法A得到的期望预测值
> - $$\mathbb{E}((g-f)^2)$$用均方误差表示回归问题的期望泛化误差
> - $$\mathbb{E}((g-\overline{g}(x))^2)$$表示算法A在不同训练集D上预测值的方差variance
> - $$\mathbb{E}((\overline{g}(x)-f)^2)$$表示期望预测值关于真实值的偏差bias
>$$
\begin{align*}
\mathbb{E}((g-f)^2)&=\mathbb{E}((g-\overline{g}(x))^2)+\mathbb{E}((\overline{g}(x)-f)^2)\\
\mathbb{E}(E_{out})&=var(x)+bias^2(x)
\end{align*}
$$

> **偏差-方差分解**可表述为：算法A在**相同容量**的不同训练集上的期望泛化误差可分解为预测值均值对真实值的**偏差**和**方差**之和。
> 
> 通过voting方法得到的集成学习器，使用G代替g，能够降低方差。

#### 2.3 Linear blending
- 原理：取所有模型预测结果的加权平均作为最终预测结果，select和voting是Linear blending的特例。

$$
G(x)=sign(\sum_{t=1}^{T} \alpha _{t}\cdot g_{t}(x)) with \alpha _{t}\geq 0
$$

- 分析：Linear blending本质上是在做特征转化（feature transformation），类似于两层学习，特征转换是在踩油门，非常容易过拟合（假设空间VC维$$\geqslant d_{vc}(\bigcup_{t=1}^{T}H_{t})$$
），因此Linear blending一般需在验证集上进行：
    - 第一层是在训练集上，得到各个基本模型$$g_{t}$$；
    - 第二层是在测试集上，将基本模型在测试集上的预测结果作为转化后的特征来训练最终结果；
    ![](/assets/blending.png)
    
#### 2.4 stacking
- 原理(特征转化-层次训练)：把基本模型的预测结果当做新的特征进行二次训练，相当于对基本模型的预测结果进行任意组合，前面所有类型都是stacking的特例。

$$
G(x)=sign(\sum_{t=1}^{T} q _{t}(x)\cdot g_{t}(x)) withq_{t}(x)\geq 0
$$

- 分析：特征转化功能强大，但非常容易过拟合
![](/assets/stacking.png)

一个富有启发的实例：
![](/assets/屏幕快照 2017-09-29 21.42.10.png)

### 为什么集成学习是有效的
![](/assets/A0B3B758-A959-469E-A5A2-05AD2ECB9704.png)

- 机器学习——开车，如何安全的开车
- 增加训练程度——油门——降低偏差——串行集成相当于做了特征转换
- 降低训练程度——刹车——降低方差——并行集成相当于做了正则化

通过合适的集成，我们可以像控制超参数旋钮一样控制模型的偏差和方差，达到优化模型性能的目的。

## 二、Bagging(Boostrap Aggregation)
> Bagging算法 （Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习算法。最初由Leo Breiman于1994年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。
### 随机森林（Random Forest）


## 三、Boosting

### AdaBoost









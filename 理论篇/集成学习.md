# 集成学习
> 集成学习（ensemble，"昂森宝"，或者aggregation）：通过组合多个学习器以获取更好的泛化性能。

## 集成学习的理论基础
集成学习的两个步骤：
1. 产生一组“弱学习器” ：$$g_{1}(\boldsymbol{x})$$、$$g_{2}(\boldsymbol{x})$$、$$g_{3}(\boldsymbol{x})$$......
2. 按照某种规则将弱分类器组合起来：$$G(\boldsymbol{x})=\sum_{t=1}^{T}\alpha_{t} g_{t}(\boldsymbol{x})$$

### 产生弱分类器的方式
> 弱分类器：泛化性能略优于随机猜测的学习器称为弱学习器。它有两层含义：
1. 弱：虽然从理论上说使用弱分类器集成足以获得好的性能，但在实践中为了减少学习器的数量，人们往往会使用比较强的学习器。
2. 强：所选用的弱分类器必须得比随机猜测要好（如二分类误分率>0.5），这样才能“强强联合”，否则该学习器会成为集成中的“害群之马”，应被过滤掉。

#### 获取弱分类器的方法
1. 基于不同的模型：$$g_{1}\in H_{1},g_{2}\in H_{2},...,g_{T}\in H_{T}$$，如通过逻辑回归和随机森林获取两个不同模型；
2. 基于的不同超参数：如通过设置随机森林不同的超参数来得到多个随机森林模型；
3. 基于不同的训练数据：如bagging的方法（有放回地均匀采样）、随机森林（有放回地均匀采样）、Adaboost（按照误分率改变样本权重）
4. 基于随机算法：如遗传编程、选取不同的seeds等

#### 集成的原则
集成原则：要获取好的集成，每个分类器应”好而不同“。

1. 好：每个分类器要有一定”准确性“，至少要好于随机猜测；
2. 不同：应保证分类器的”多样性“，通过上述不同的方式获取到区分度较高的基本分类器；

假设各分类器相互独立，则集成错误率随基本学习器数量的增加而指数级下降：

$$
P(G(x)\neq f(x))\leqslant exp(-\frac{1}{2}T(1-2\varepsilon )^2)
$$

”准确性“和”独立性“的矛盾：当准确性很高以后，要增加准确性就得牺牲掉多样性，如何产生”好而不同“的多个学习器正是集成学习的核心。

### 弱分类器集成的方式
假设我们已经获取到了多个基本学习器$$g_{1}(\boldsymbol{x})$$、$$g_{2}(\boldsymbol{x})$$、$$g_{3}(\boldsymbol{x})$$......，通常有以下几种方式来进行集成：

1. select:在所有模型中选取验证误差最小的模型作为最终模型。
$$
G(x)=g_{*} with t_{*}=argmin(E_{val}(g^-_{t}))
$$
2. uniform blending(voting)：取所有模型的均匀组合作为最终模型。
$$
G(x)=sign(\sum_{t=1}^{T}g_{t}(x))
$$
3. Linear blending：取所有模型的线性组合作为最终模型
$$

$$
4. stacking：按照输入有条件的进行组合

#### select
rely on one strong hypothesis  

#### uniform blending(voting)



#### Linear blending

#### stacking






















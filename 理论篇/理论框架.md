
1.  机器学习的目的是找到泛化误差最小的模型，学习问题可以分解为两个基本问题：
    1. 使经验误差最小，这可以通过经验风险最小来达到；
    2. 使经验误差尽可能接近泛化误差，这主要受到样本容量和模型复杂度的影响。样本容量就是数据量的多少，模型复杂度可以通过假设空间的VC维来衡量。
2. 与两个基本问题相对应，我们可以将算法的期望泛化误差分解为偏差和方差两部分。偏差-方差分解是分析影响机器学习系统性能的因素的最重要工具，机器学习性能不好的原因要么可归因于高偏差（欠拟合）要么可归因于高方差（过拟合），偏差-方差分解可以比作车速仪表器，相应的各种调节机器学习系统性能的措施可以比作是在踩油门（减小偏差）或踩刹车（减小方差）。
3. 学习曲线描述了经验误差和泛化误差随样本容量增加的变化趋势，通过学习曲线可以识别出增加样本容量是否能够改善机器学习系统的性能。
4. 正则项是结构化风险最小化的等价形式

## 一、形式化学习问题（Why can machine learn?）
**监督学习**：从带标签的训练集中识别出特征到标签的某种模式g，作为理想模式的近似。

**经验风险最小化(empirical risk minimization,ERM)**：通过算法A，从假设空间H中选取一个在训练集D上的经验误差最小的模型g，作为我们的最终模型。

![](/assets/tdml.png)

- 目标函数(Target Function)：$$y = f(x)$$是特征到标记的理想模式(未知)，$$(x_{i},y_{i})$$是数据集中的观测样例，$$y_{i}=f(x_{i})+\zeta $$，$$\zeta$$表示噪声；
- 假设函数（Hypothesis Function）：$$y = h(x)$$是假设空间$$H=\{h:X \to Y  | h = h(x)\}$$中的某个模型；
- 损失函数（Loss Function）：$$L=L(y,h(x))$$，计算样本预测值和观测值的误差；
- 经验误差（Training error）：在训练集上损失函数的均值
$$
E_{in}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i}))
$$

- 泛化误差（Generalization error）：在测试集上损失函数的期望/均值，在做交叉验证时将验证误差作为泛化误差的近似。
$$
E_{out}(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))]
$$

**学习是否可行的问题可以转化为以下两个核心问题**：
1. 经验误差是否足够小：通过经验风险最小化来达到
$$
g= \underset{h\in H}{argmin} E_{in}(h)
$$
2. 泛化误差是否足够接近经验误差：由样本容量和模型复杂度决定
$$
E_{out}(g) \approx E_{in}(g)\approx 0
$$

## 二、VC维
> VC维（Vapnik-Chervonenkis Dimension，由研究人员Vapnik和Chervonenkis在1958年发现）：对于一个指示函数集H，如果存在k个样本，它的$$2^k$$种划分中的任一种都能够被H中的函数f分开，则称H能够把k个样本打散(shatter)，函数集的VC维就是它能shat

的概念是为了研究学习过程一致收敛的速度和推广性，由统计学理论定义的有关函数集学习性能的一个重要指标

由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

由该不等式可知，对于单个假设来说，经验误差与泛化误差间的差异随着样本容量的增长而指数级衰减。但当有多个假设时会恶化算法的泛化性能（can get worse when involving choice），假设空间中存在某个假设使得训练误差和泛化误差的偏差大于$$\epsilon$$的概率为：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

经验误差和泛化误差的差异，随训练集样本数的增加而指数级衰减，随假设空间容量增加而增加。
### 1. VC维
#### VC维-提出背景
存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的，此时根据该不等式无法给出经验误差和泛化误差的一个确定的上界。事实上，在放大不等式右侧的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集，也就是说它被过度放大了。我们希望能够找到一个多项式级别的函数来代替|H|用于描述假设空间的复杂度。

![](/assets/ee.png)

#### VC维-相关概念和推导
首先来介绍一些假设空间上的概念：

- **成长函数**（Growth Function）：给定假设空间H，H中有效模型个数与样本容量m的函数关系（H中所有将样本集划分为相同结果的模型都被看作是同一个有效模型），记做$$m_{H}(m)$$。对于样本数为m的二分类问题，其有效模型数的一个上界为$$2^{m}$$，但是这个指数上界仍然过于宽泛了。下面我们的任务是将其缩减到一个多项式级别的函数。

![](/assets/34.png)

- **打碎（Shatter）**：给定一个具有确定分布的样本集D和假设空间H，如果D的任何划分都能被H划分，则说D能够被H打碎。下面第一个样本集可以被二维感知机shatter，第二个样本集不可以。

![](/assets/ss.png)
![](/assets/shatter.png)

- **突破点（Break Point）**：给定假设空间H，如果任意分布的k个样本都不能被H打碎，则称k为H的突破点。突破点仅与假设空间有关，与样本分布和划分无关。容易证明，如果k是H的突破点则任意比k大的整数也是H的突破点。

![](/assets/123.png)

- **VC维（Vapnik-Chervonenkis Dimension）**：对于给定的假设空间H，它所能shatter的最大样本数，称该假设空间的VC维。


- **上限函数（Bounding function）**：是指当假设空间H的最小突破点为k时，H的成长函数的最大值，记做B(N,k)。

$$
B(N,k)\leqslant B(N-1,k)+B(N-1,k-1)\\
m_{H}(N)\leqslant B(N,k)\leqslant \sum_{i=0}^{k-1}C_{N}^{i}\leqslant N^{k-1}
$$

 如果假设空间H的VC维为$$d_{vc}$$（$$d_{vc}=k-1$$），那么假设空间成长函数的一个上限是$$N^{d_{vc}}$$，使用假设空间的VC维来替换原不等式中的假设空间容量|H|，可得到：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant 4m_{H}(2N)exp(-\frac{1}{8}\epsilon ^{2}N)\\
&\leqslant 4N^{d_{vc}}exp\left ( -\frac{1}{8}\epsilon ^{2} N \right )
\end{align*}
$$

#### VC维-结论
VC维是描述假设空间的一个核心概念，关于VC维我们可以得到以下一些结论：

1. VC维等于假设空间最小突破点减1；
2. VC维反应了函数集的学习能力，VC维越大函数集的学习能力越强；
3. VC维反映了模型复杂度，VC维越大模型就越复杂；
4. 目前尚未有通用的方法来计算任意函数集的VC维，只能计算一些特殊的函数集的VC维，如N维空间中线性分类器的VC维是N+1；
5. 一个有效的实践规律：V维与假设参数w的自由变量的个数大致相等；
6. 如果假设空间

### 2. 误差和样本数量
**学习曲线（Learning curve）**：学习曲线反映了经验误差和泛化误差与训练集样本数量的关系。

![](/assets/20140924215356547.jpeg)

- 当模型比较简单时，对训练集数据的拟合程度较低，无论是经验误差还是泛化误差都很大（发生欠拟合）。随着样本数量的增加，经验误差略有增大，泛化误差和经验误差的差距指数级衰减。
- 当模型比较复杂时，训练误差在小于假设空间的VC维时，可以将样本完全分离，此时训练误差为0，此时泛化误差和经验误差差距巨大导致泛化误差很大，发生过拟合；随着样本数量的增加，泛化误差和经验误差的差距逐渐变小。因为我们的目的是得到小的 Eout，而当我们有大量数据的时候复杂模型的 Eout 比简单模型的 Eout 要小，所以应该优先选择复杂的模型。

### 3. 误差和假设空间
将原不等式做一个改写：

![](/assets/vc_power1.png)

不等式右侧第一项表示经验误差，第二项表示模型复杂度。
- 模型复杂度随VC维的增加而增加
- 经验误差和泛化误差的差距随VC维的增加而增加
- 当假设空间的VC维很小时，学习器的学习能力较差，经验误差较大，可能发生欠拟合
- 昂假设空间的VC维很大时，学习器的学习能力很强，经验误差较小，但因为泛化误差和经验误差差距较大，导致泛化误差过大，发生过拟合

![](/assets/vcc.png)

### 4. 假设空间和样本容量

![](/assets/dvc_N.png)

如果给定误差阈值$$\epsilon$$和精度$$\delta$$，对$$\delta$$进行简化，得到$$N^{d_{vc}}e^{-N}$$，将其画出图像观察其变化趋势：

![](/assets/bb.png)

因此我们可以看到在横线所在位置，N的增加和dvc几乎是一种线性的关系。经过工业界足够的现象表明，我们通常只要使得N≥10dvc就足够了。

### 5. 偏差-方差分解
偏差-方差分解与“误差和假设空间”相对应，其中经验误差反映了算法的偏差，模型的复杂度反映了算法的方差，VC维反映了训练程度。

一般算法的**期望泛化误差**可分解为偏差、方差和噪声之和（理论评估）：

$$
E(f;D) = bias^{2}(x) + var(x) + \varepsilon ^{2}
$$

- 偏差：度量了期望预测结果与真实结果的偏离程度，即**算法本身**的拟合能力；
- 方差：度量了同等大小的训练集变动导致的学习性能的变化，即**数据扰动**造成的影响；
- 噪声：度量了当前任务下任何学习算法所能达到的期望泛化误差的下界，即问题**本身的难度**；

![](/assets/2-1.png)

结论：
- 泛化性能是由算法的拟合能力、数据的充分性、任务本身的难度共同决定的（**三因素**：算法-数据-噪声）；给定学习任务，为获取较好的泛化能力，可以从以下三个角度来思考：
    - 数据降噪：保证数据质量
    - 选择合适的学习算法：充分拟合数据
    - 扩大数据规模：保证数据数量，使得数据扰动产生的影响较小
- 偏差-方差窘境：偏差和方差是有冲突的
    - 当训练不足时，学习器的拟合能力不强，数据扰动不足以影响泛化能力，偏差主导了泛化误差，发生欠拟合；
    - 随着训练程度的加深，学习器的拟合能力逐渐加强，数据扰动渐渐能被学习器学到
    - 当训练程度充足后，学习器的拟合能力非常强，数据的轻微扰动都会导致学习器发生显著变化，方差主导了泛化误差。若数据自身的非全局特性被学习器学到则发生过拟合；

![](/assets/b-v.jpeg)


## 三、正则化



## 结论
学习的过程就是从假设空间中选择一个经验误差最小的模型来作为理想模型的近似。这之所以可能，取决于两个条件：经验误差是否足够小，经验误差是否足够接近泛化误差。这两个条件是否成立主要取决于两个因素：样本容量、模型复杂度。样本容量m越大，经验误差就越接近于泛化误差；模型复杂度可以用假设空间的VC维来描述，VC维又可以近似等于模型中自由参数的个数，模型越复杂，经验误差就越小，但是经验误差和泛化误差的差别也会越来越大，但是当VC维有限而样本足够大时（样本数大于10倍的VC维），就可以保证经验误差和泛化误差差距足够小。























## 决策树模型
> 决策树：决策树是一种对实例进行分类/回归的树形结构。每个内部节点表示一次特征测试，每条边表示特征测试结果，每个叶节点表示一种最终决策结果，从根节点到叶节点的每条路径表示一条判定规则，对每个实例都存在唯一的路径与之对应。

1. if-then规则视角：决策树可看做是一个if-then规则的集合。每条规则唯一对应一条从根节点到叶节点的路径，内部节点是规则的条件，叶节点是规则的结论。if-then规则集是一种stacking方法——按条件为每个模型赋权。
$$
G(x) = \sum_{t=1}^{T}q_{t}(x)\cdot g_{t}(x)\\
G(x) = \sum_{t=1}^{T}I(x \text{ on path t})\cdot leaf_{t}(x)
$$

2. 特征空间视角：决策树也可以看做是在特征空间中的条件概率分布。按各维特征的不同取值将特征空间划分为互不相交的区域$$R_{1},R_{2}...$$，在每个区域定义一个类的概率分布就构成可一个条件概率分布$$P(y|X)$$。
![](/assets/image00381.jpeg)

3. 递归视角：决策树可以进行递归定义
    - $$G(x)$$全树模型：x在决策树中的判定结果
    - $$b(x)$$分支条件：x在某特征上的测试结果
    - $$G_{c}(x)$$子树模型：x在子树上的判定结果
$$
G(x) = \sum_{t=1}^{T}I(b(x)=c)\cdot G_{c}(x)
$$

## 决策树学习
决策树学习包括两个部分：

1. 决策树生成：
    1. 特征选择-切分方式选择：ID3、C4.5、CART
    2. 基本模型：对于分类问题，取众数；对于回归问题，取平均数
    3. 退出条件：D中标记一样、D中特征一样、A属性为空、D数据集为空
2. 决策树剪枝

通用算法：

$$
G(x) = \sum_{t=1}^{T}I(b(x)=c)\cdot G_{c}(x)
$$


- 输入：训练数据集D，特征集A，阈值$$\epsilon $$；
- 输出：决策树T

(1) 如果达到退出条件，返回基本函数
    退出条件包括：
1. D中所有样本的y都相同——把y作为叶节点返回
2. D中所有样本的X都相同——把X中所含样本最多的类别y最为叶节点返回
3. D为空，以父节点中样本数最多的类别y作为叶节点返回
4. 特征为空，以当前D中样本数最多的类别y作为叶节点返回

```python
输入：训练数据集D，特征集A，阈值$$\epsilon $$
输出：
```





















## 感知机（perceptron）

- 输入空间：$$\chi \subseteq R^{n}$$
- 输出空间：$$Y = \{+1,-1\}$$


### 1. 学习模型
- 假设空间：
$$
f(x)=sign(wx+b)\\
sign(x) = \begin{cases}
+1 & \text{ if } x\geq 0 \\ 
 -1 & \text{ if } x< 0
\end{cases}
$$

- 感知机与神经网络：perceptron是神经网络的基础单元。感知机以一个实值向量作为输入，计算这些输入的线性组合，如果结果大于某阈值就输出+1，否则输出-1。w称作**权值向量**(weight vector)，b称作**偏置**(bias)。

![](/assets/perceptron-1.png)

- 感知机与SVM：感知机模型是定义在特征空间的**线性分类器**（linear classifier）。线性方程$$wx+b=0$$对应于特征空间的一个分离超平面S，w是超平面的**法向量**，b是超平面的**截距**，超平面将特征空间划分为两个部分，分别对应为正负例。

![](/assets/perceptron-2.png)

### 2. 学习策略
- 损失函数：如果将误分类点数作为损失函数，对w，b不可导不易优化。因此选择误分类点到超平面S的总距离作为损失函数：

特征空间中任一点到超平面$$wx+b=0$$的距离为：
$$
\frac{|wx+b|}{||w||} 
$$

对于误分类点$$(x_{i},y_{i})$$，到超平面距离也可表示为：

$$
\frac{-y_{i}(wx_{i}+b)}{||w||} 
$$

因此，感知机的损失函数可以表示为：

$$
L(w,b) = -\sum_{x_{i}\in M} y_{i}(wx_{i}+b)
$$

### 3. 学习算法
目标函数：
$$
\underset{w,b}{min}L(w,b) = -\sum_{x_{i}\in M} y_{i}(wx_{i}+b)
$$

- 随机梯度下降法：每次在误分类点中随机选取一个点使目标函数梯度下降，直至不再有误分类点或者迭代次数达到上限停止。

损失函数的梯度：

$$
\frac{\partial L}{\partial w} = -\sum_{x_{i}\in M} y_{i}x_{i}\\
\frac{\partial L}{\partial b} = -\sum_{x_{i}\in M} y_{i}
$$

每次迭代，随机选取误分类点$$(x_{i},y_{i})$$：

$$
w\leftarrow w+\eta y_{i}x_{i}\\
b\leftarrow b+\eta y_{i}
$$

- 感知机学习算法：

















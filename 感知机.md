## 感知机（perceptron）

- 输入空间：$$\chi \subseteq R^{n}$$
- 输出空间：$$Y = \{+1,-1\}$$


### 1. 学习模型
- 假设空间：
$$
f(x)=sign(wx+b)\\
sign(x) = \begin{cases}
+1 & \text{ if } x\geq 0 \\ 
 -1 & \text{ if } x< 0
\end{cases}
$$

- 感知机与神经网络：perceptron是神经网络的基础单元。感知机以一个实值向量作为输入，计算这些输入的线性组合，如果结果大于某阈值就输出+1，否则输出-1。w称作**权值向量**(weight vector)，b称作**偏置**(bias)。

![](/assets/perceptron-1.png)

- 感知机与SVM：感知机模型是定义在特征空间的**线性分类器**（linear classifier）。线性方程$$wx+b=0$$对应于特征空间的一个分离超平面S，w是超平面的**法向量**，b是超平面的**截距**，超平面将特征空间划分为两个部分，w指向的一侧为正例，$$y_{i}(wx_{i}+b)\geqslant 0$$；另一侧为负例，$$y_{i}(wx_{i}+b)< 0$$。

![](/assets/perceptron-2.png)

- 函数间隔（function margin）：$$\gamma_{i} = y_{i}(wx_{i}+b)$$
 - 函数间隔的正负表示分类的正确性，函数间隔为正说明该样本点能够被分隔超平面正确分类；
 - 函数间隔的绝对值表示分类的确定程度(法向量模为1)，从几何意义上函数间隔绝对值表示样本点到分隔超平面的距离；


### 2. 学习策略
- 损失函数：如果将误分类点数作为损失函数，对w，b不可导不易优化。因此选择误分类点到超平面S的总距离作为损失函数：

特征空间中任一点到超平面$$wx+b=0$$的距离为：
$$
\frac{|wx+b|}{||w||} 
$$

对于误分类点$$(x_{i},y_{i})$$，到超平面距离也可表示为：

$$
\frac{-y_{i}(wx_{i}+b)}{||w||} 
$$

因此，感知机的损失函数可以表示为：

$$
L(w,b) = -\sum_{x_{i}\in M} y_{i}(wx_{i}+b)
$$

### 3. 学习算法
#### 原始形式
目标函数：
$$
\underset{w,b}{min}L(w,b) = -\sum_{x_{i}\in M} y_{i}(wx_{i}+b)
$$

- 随机梯度下降法：每次在误分类点中随机选取一个点使目标函数梯度下降，直至不再有误分类点或者迭代次数达到上限停止。

损失函数的梯度：

$$
\frac{\partial L}{\partial w} = -\sum_{x_{i}\in M} y_{i}x_{i}\\
\frac{\partial L}{\partial b} = -\sum_{x_{i}\in M} y_{i}
$$

- 感知机学习算法原始形式：输入：训练数据集，学习率$$\eta \in (0,1]$$

（1） 初始化w,b
（2） 在训练集中随机选一个误分类点$$(x_{i},y_{i})$$

$$
w\leftarrow w+\eta y_{i}x_{i}\\
b\leftarrow b+\eta y_{i}
$$

（3）循环第二步直至不再有误分类点，求得最优解时的参数$$w^{*}$$，$$b^{*}$$

感知机学习算法由于采用不同的初值或选取不同的误分类点，可以有无限多个解。

#### 对偶形式
如果将w，b的初值设置为0，则可以将w，b表示为实例$$x_{i}$$和标记$$y_{i}$$的线性组合。设$$n_{i}$$为第i个样本被误分类的次数，令$$\alpha _{i}=n_{i}\eta $$，则

$$
w = \sum_{i=1}^{N}\alpha _{i}y_{i}x_{i}\\
b=\sum_{i=1}^{N}\alpha _{i}y_{i}
$$

- 感知机学习算法对偶形式：输入：训练数据集，学习率$$\eta \in (0,1]$$。输出：$$\alpha $$，b；感知机模型$$f(x)=sign\left ( \sum_{j=1}^{N}\alpha _{j}y_{j}x_{j}\cdot x + b \right )$$

（1）$$\alpha \leftarrow 0$$，$$b \leftarrow 0$$
（2）在训练集中选取误分类点$$(x_{i},y_{i})$$

$$
\alpha_{i}\leftarrow \alpha_{i}+\eta\\
b\leftarrow b+\eta y_{i}
$$

（3）循环第二步直至不再有误分类点，求得最优解时的参数$$\alpha^{*}$$，$$b^{*}$$

- Gram矩阵:对偶形式中训练实例仅以内积形式出现，可以预先将实例间的内积计算出来并以矩阵形式存储，该矩阵称为Gram矩阵

$$
Gram = [x_{i}\cdot x_{j}]_{N\times N}
$$

### 4. 数据集线性可分及算法收敛性
- 线性可分：对于给定数据集，如果存在某个超平面S，能够将数据集中的正负例点完全分隔到两侧，或者说数据集中所有样本点的函数间隔均为正，则说该数据集是线性可分的（linearly separable data set）。

- 对于线性可分的数据集，感知机算法收敛：

将偏置b并入权重w，$$\widehat{w}=(w^{T},b)^{T}$$，$$\widehat{x}=(x^{T},1)^{T}$$，则：

$$
\widehat{w}\cdot \widehat{x} = wx+b
$$

感知机在数据集上的误分类次数满足：

$$
k\leqslant \left ( \frac{R}{\gamma } \right )^{2}
$$

其中，$$||\widehat{w}||=1$$
- $$\gamma =\underset{i}{min} (y_{i}\widehat{w}\widehat{x}_{i})$$，代表数据集与分隔超平面的最短距离
- $$R = \underset{i}{max} ||\widehat{x}_{i}||$$，代表数据集的半径。















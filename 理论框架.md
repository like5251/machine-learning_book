> ## Why can Machines Learn?

## 一、形式化学习问题
**监督学习**：从带标签的训练集中识别出从特征到标签的某种模式，作为理想模式的近似。

**学习过程**：通过算法A，从假设空间中选取一个模型g，使得g在训练集D上的经验误差最小，将g作为理想模型f的近似。

![](/assets/tdml.png)

- 目标函数(Target Function)：$$y = f(x)$$，是未知的特征到标记的理想模式，$$y_{i}=f(x_{i})+\zeta $$，其中$$(x_{i},y_{i})$$是数据集中的样例，$$\zeta$$表示噪声
- 假设函数（Hypothesis Function）：$$y = h(x)$$是假设空间$$H=\{h:X \to Y  | h = h(x)\}$$中的某个模型
- 损失函数（Loss Function）：$$L=L(y,h(x))$$，计算样本预测值和真实值的误差

- 经验误差（Training error）：

$$
R_{emp}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i}))
$$

- 泛化误差（Generalization error）：

$$
R(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))]
$$

**学习可行的两个核心条件**：
1. 经验误差足够小：对应于训练阶段，模型对训练集有很好拟合
2. 泛化误差足够接近经验误差：对应于测试阶段，模型对新样本也能很好拟合

$$
R(h) \approx R_{emp}(h)\approx 0
$$

## 泛化界限
### 无限样本集
由大数定理：

$$
\lim_{m \to\infty  }P[|E[X]-\overline{X}|>\epsilon ] = 0
$$

应用在泛化误差上：

$$
\lim_{m \to\infty  }P[|R(h)-R_{emp}(h)|>\epsilon ] = 0
$$

> 结论：当训练集足够大时，泛化误差依概率等于经验误差

### 有限样本集
由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

> 结论：对假设空间中任一假设，训练误差与泛化误差的差异随数据集大小成指数级衰减

### 最坏假设
以上只是讨论了样本空间中任何单一的假设对应的经验误差和泛化误差的差异，我们更关注在做出最坏假设的情形下经验误差是否还能近似等于泛化误差。（假设空间中存在经验误差与泛化误差间的偏差大于$$\epsilon$$的概率）

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

> 结论：对于给定的假设空间，最坏情况下，训练误差和泛化误差的差异随训练集增大而指数型衰减，但随假设空间增大而同比增大。



生长函数（growth function）：假设空间的大小关于数据集大小的函数。对于二分类问题，生长函数的上界为：

$$
\bigtriangleup _{H}(m)\leqslant 2^{m}
$$

指数上界仍然太大，我们希望能找到一个多项式级别的生长函数上界。

突破点（break point）:空间H不能打碎（shatter）的最小点数。如感知机最小不能打碎4个点，突破点为4。

上界函数（Bounding Function）：长度为N的向量中满足任意k个分量都不能被打碎（全部出现）的向量个数，定义为B(N,k)


































> ## Why can Machines Learn?

## 一、形式化学习问题
**监督学习**：从带标签的训练集中识别出从特征到标签的某种模式，作为理想模式的近似。

**学习过程**：通过算法A，从假设空间中选取一个模型g，使得g在训练集D上的经验误差最小，将g作为理想模型f的近似。

![](/assets/tdml.png)

- 目标函数(Target Function)：$$y = f(x)$$，是未知的特征到标记的理想模式，$$y_{i}=f(x_{i})+\zeta $$，其中$$(x_{i},y_{i})$$是数据集中的样例，$$\zeta$$表示噪声
- 假设函数（Hypothesis Function）：$$y = h(x)$$是假设空间$$H=\{h:X \to Y  | h = h(x)\}$$中的某个模型
- 损失函数（Loss Function）：$$L=L(y,h(x))$$，计算样本预测值和真实值的误差

- 经验误差（Training error）：

$$
R_{emp}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i}))
$$

- 泛化误差（Generalization error）：

$$
R(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))]
$$

**学习可行的两个核心条件**：
1. 经验误差足够小：对应于训练阶段，对应于算法的偏差
2. 泛化误差足够接近经验误差：对应于测试阶段，对应于算法的方差

$$
R(h) \approx R_{emp}(h)\approx 0
$$

## 二、泛化界限
泛化误差的偏差-方差分解描述了算法偏差-算法方差-泛化误差随训练程度的变化情况，本质上是本文内容的定性版本。
- 算法偏差对应于经验误差，即算法对训练集的拟合程度，模型复杂度越高，算法偏差一般越小
- 算法方差对应于经验误差和泛化误差的差异，即数据扰动的影响，算法方差随样本容量增大指数级衰减，但会随模型复杂度而增大
- 训练程度对应于模型复杂度，即假设空间的大小，或说模型参数数量，或说算法的类型

$$
R\leqslant R_{emp}+\Omega (m,|H|,\epsilon )\\
E(f;D) = bias^{2}(x) + var(x) + \varepsilon ^{2}
$$

### 单一假设——无限样本集
由大数定理：

$$
\lim_{m \to\infty  }P[|E[X]-\overline{X}|>\epsilon ] = 0
$$

应用在泛化误差上：

$$
\lim_{m \to\infty  }P[|R(h)-R_{emp}(h)|>\epsilon ] = 0
$$

> 结论：对假设空间中任一假设，当训练集足够大时，泛化误差依概率等于经验误差

### 单一假设——有限样本集
由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

> 结论：对假设空间中任一假设，训练误差与泛化误差的差异随数据集大小成指数级衰减

### 多个假设
当有多个假设时会恶化算法的泛化性能（can get worse when involving choice），假设空间中存在某个假设使得训练误差和泛化误差的偏差大于$$\epsilon$$的概率为：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

> 结论：对于给定的假设空间，训练误差和泛化误差的差异随训练集增大而指数型衰减，但随假设空间增大而同比增大。


### 有穷假设空间
#### 样本容量
如果给定误差阈值$$\epsilon$$和精度$$\delta$$，我们需要多大的样本量才能保证训练误差和泛化误差差距不超过$$\epsilon$$的最小概率为$$1-\delta$$：

$$
m\geqslant \frac{1}{2\epsilon ^{2}}In\frac{2|H|}{\delta }
$$

上面不等式确定了m的一个下界，称为算法的样本复杂度。

#### 模型复杂度
问题|\|H\|很小|\|H\|很大
---|---|---
R ≈ Remp|满足|不满足
Remp≈0|不满足|满足

![](/assets/com.jpeg)

### 无穷假设空间
存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的。此时下式就无法给出经验误差和泛化误差的一个确定的上界：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

- 原因：以上不等式的上界被过度放大了，原因是在放大的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集

![](/assets/ee.png)

- 成长函数（Growth Function）：假设空间中有效模型的个数与样本容量的函数关系（将所有划分结果相同的模型看作是同一个模型），记做$$m_{H}(N)$$。成长函数取决于假设空间和样本分布。我们可以使用成长函数来代替不等式中的|H|来估计经验误差和泛化误差的差距。对于样本数为m的二分类问题，其有效模型数的一个上界为$$2^{m}$$，但是这个指数上界仍然过于宽泛了。下面我们的任务是将其缩减到一个多项式级别的函数。

![](/assets/34.png)

- **打碎（Shatter）**：对某个分布的样本集D和假设空间H，如果D的任何划分都能被H划分，则说该分布下的D能够被H打碎。下面第一个样本集可以被二维感知机shatter，第二个样本集不可以。

![](/assets/ss.png)
![](/assets/shatter.png)

- 突破点（Break Point）：对于假设空间H和整数k，如果任意分布的k个样本都不能被H打碎，则称k为H的突破点。突破点仅与假设空间有关，与样本**分布**和**划分**无关。很容易证明，如果k是H的突破点则任意比k大的整数也是H的突破点。

![](/assets/123.png)

####  VC维

- VC维：对于给定的假设空间H，它所能shatter的最大样本容量，称该假设空间的VC维。

VC维反应了函数集的学习能力，VC维越大学习能力越强，模型越复杂。遗憾的是目前尚没有通用的关于任意函数集VC维的计算理论，只能对一些特殊的函数集计算其VC维。例如N维空间中线性分类器的VC维是N+1。

> H的VC维=H的最小突破点-1

- 上限函数（Bounding function）：B(N,k)，假设空间H的最小突破点为k时，H的成长函数的最大值。

$$
B(N,k)\leqslant B(N-1,k)+B(N-1,k-1)
$$

那么我们可以明显看出这是一个二项式系数，可得到通项公式：

$$
m_{H}(N)\leqslant B(N,k)\leqslant \sum_{i=0}^{k-1}C_{N}^{i}\leqslant N^{k-1}
$$

> 该式说明，如果假设空间H的VC维为k，假设空间成长函数的一个上限是$$N^{k}$$。经验误差和泛化误差的差距可表示为：

$$
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 4m_{H}(2N)exp(-\frac{1}{8}\epsilon ^{2}N)\leqslant 4N^{k}exp\left ( -\frac{1}{8}\epsilon ^{2} N \right )
$$
































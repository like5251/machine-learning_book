> ## Why can Machines Learn?

## 一、形式化学习问题
**监督学习**：从带标签的训练集中识别出从特征到标签的某种模式。

**学习过程**：通过算法A，从假设空间中选取一个模型g，使得g在训练集D上的经验误差最小，将g作为理想模型f的近似。

![](/assets/tdml.png)

- 目标函数(Target Function)：$$y = f(x)$$，是未知的特征到标记的理想模式，$$y_{i}=f(x_{i})+\zeta $$，其中$$(x_{i},y_{i})$$是数据集中的样例，$$\zeta$$表示噪声
- 假设函数（Hypothesis Function）：$$y = h(x)$$是假设空间$$H=\{h:X \to Y  | h = h(x)\}$$中的某个模型
- 损失函数（Loss Function）：$$L=L(y,h(x))$$，计算样本预测值和真实值的误差

- 经验误差（Training error）：

$$
R_{emp}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i}))
$$

- 泛化误差（Generalization error）：

$$
R(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))]
$$

**学习是否可行可以转化为以下两个核心问题**：
1. 经验误差是否足够小
2. 泛化误差是否足够接近经验误差

$$
R(h) \approx R_{emp}(h)\approx 0
$$

## 二、泛化界限
由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

由该不等式可知，对于单个假设来说，经验误差与泛化误差间的差异随着样本容量的增长而指数级衰减。但当有多个假设时会恶化算法的泛化性能（can get worse when involving choice），假设空间中存在某个假设使得训练误差和泛化误差的偏差大于$$\epsilon$$的概率为：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

经验误差和泛化误差的差异，随训练集样本数的增加而指数级衰减，随假设空间容量增加而增加。
### VC维
#### VC维-提出背景
存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的，此时根据该不等式无法给出经验误差和泛化误差的一个确定的上界。事实上，在放大不等式右侧的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集，也就是说它被过度放大了。我们希望能够找到一个多项式级别的函数来代替|H|用于描述假设空间的复杂度。

![](/assets/ee.png)

#### VC维-相关概念和推导
首先来介绍一些假设空间上的概念：

- **成长函数**（Growth Function）：给定假设空间H，H中有效模型个数与样本容量m的函数关系（H中所有将样本集划分为相同结果的模型都被看作是同一个有效模型），记做$$m_{H}(m)$$。对于样本数为m的二分类问题，其有效模型数的一个上界为$$2^{m}$$，但是这个指数上界仍然过于宽泛了。下面我们的任务是将其缩减到一个多项式级别的函数。

![](/assets/34.png)

- **打碎（Shatter）**：给定一个具有确定分布的样本集D和假设空间H，如果D的任何划分都能被H划分，则说D能够被H打碎。下面第一个样本集可以被二维感知机shatter，第二个样本集不可以。

![](/assets/ss.png)
![](/assets/shatter.png)

- **突破点（Break Point）**：给定假设空间H，如果任意分布的k个样本都不能被H打碎，则称k为H的突破点。突破点仅与假设空间有关，与样本分布和划分无关。容易证明，如果k是H的突破点则任意比k大的整数也是H的突破点。

![](/assets/123.png)

- **VC维（Vapnik-Chervonenkis Dimension）**：对于给定的假设空间H，它所能shatter的最大样本数，称该假设空间的VC维。


- **上限函数（Bounding function）**：是指当假设空间H的最小突破点为k时，H的成长函数的最大值，记做B(N,k)。

$$
B(N,k)\leqslant B(N-1,k)+B(N-1,k-1)\\
m_{H}(N)\leqslant B(N,k)\leqslant \sum_{i=0}^{k-1}C_{N}^{i}\leqslant N^{k-1}
$$

 如果假设空间H的VC维为$$d_{vc}$$（$$d_{vc}=k-1$$），那么假设空间成长函数的一个上限是$$N^{d_{vc}}$$，使用假设空间的VC维来替换原不等式中的假设空间容量|H|，可得到：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant 4m_{H}(2N)exp(-\frac{1}{8}\epsilon ^{2}N)\\
&\leqslant 4N^{d_{vc}}exp\left ( -\frac{1}{8}\epsilon ^{2} N \right )
\end{align*}
$$

#### VC维-结论
VC维是描述假设空间的一个核心概念，关于VC维我们可以得到以下一些结论：

1. VC维等于假设空间最小突破点减1；
2. VC维反应了函数集的学习能力，VC维越大函数集的学习能力越强；
3. VC维反映了模型复杂度，VC维越大模型就越复杂；
4. 目前尚未有通用的方法来计算任意函数集的VC维，只能计算一些特殊的函数集的VC维，如N维空间中线性分类器的VC维是N+1；
5. 一个有效的实践规律：V维与假设参数w的自由变量的个数大致相等；
6. 如果假设空间


### 









## 二、泛化界限
泛化误差的偏差-方差分解描述了算法偏差-算法方差-泛化误差随训练程度的变化情况，本质上是本文内容的定性版本。
- 算法偏差对应于经验误差，即算法对训练集的拟合程度，模型复杂度越高，算法偏差一般越小
- 算法方差对应于经验误差和泛化误差的差异，即数据扰动的影响，算法方差随样本容量增大指数级衰减，但会随模型复杂度而增大
- 训练程度对应于模型复杂度，即假设空间的大小，或说模型参数数量，或说算法的类型

$$
R\leqslant R_{emp}+\Omega (m,|H|,\epsilon )\\
E(f;D) = bias^{2}(x) + var(x) + \varepsilon ^{2}
$$

### 单一假设——无限样本集
由大数定理：

$$
\lim_{m \to\infty  }P[|E[X]-\overline{X}|>\epsilon ] = 0
$$

应用在泛化误差上：

$$
\lim_{m \to\infty  }P[|R(h)-R_{emp}(h)|>\epsilon ] = 0
$$

> 结论：对假设空间中任一假设，当训练集足够大时，泛化误差依概率等于经验误差

### 单一假设——有限样本集
由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

> 结论：对假设空间中任一假设，训练误差与泛化误差的差异随数据集大小成指数级衰减

### 多个假设
当有多个假设时会恶化算法的泛化性能（can get worse when involving choice），假设空间中存在某个假设使得训练误差和泛化误差的偏差大于$$\epsilon$$的概率为：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

> 结论：对于给定的假设空间，训练误差和泛化误差的差异随训练集增大而指数型衰减，但随假设空间增大而同比增大。


### 有穷假设空间
#### 样本容量
如果给定误差阈值$$\epsilon$$和精度$$\delta$$，我们需要多大的样本量才能保证训练误差和泛化误差差距不超过$$\epsilon$$的最小概率为$$1-\delta$$：

$$
m\geqslant \frac{1}{2\epsilon ^{2}}In\frac{2|H|}{\delta }
$$

上面不等式确定了m的一个下界，称为算法的样本复杂度。

#### 学习曲线
学习曲线是指Ein和Eout与样本数量的关系曲线。

![](/assets/20140924215356547.jpeg)

显然，当一个模型比较简单的时候，无论 Eout 还是 Ein 都很大，随着 N 的增大，Eout 与 Ein 的距离变得很小。相反，当一个模型很复杂的时候，Ein 在一开始的时候甚至可以为0（想象 VC 的情况，当N 小于 VC 维是，我们可以完全的分离样本内的数据，不会产生误差），但是当 N比较小的时候，Eout 变得非常大， 那是因为小 N 不足以应付复杂的复杂的模型。当N
不断增大的时候，Eout 与 Ein 靠拢，但是 Eout 与 Ein 的距离却比简单模型的要到。但是因为我们的目的的得到小的 Eout，而当我们有大量数据的时候复杂模型的 Eout 比简单模型的 Eout 要小，所以应该优先选择复杂的模型
#### 模型复杂度
问题|\|H\|很小|\|H\|很大
---|---|---
R ≈ Remp|满足|不满足
Remp≈0|不满足|满足

![](/assets/com.jpeg)

### 无穷假设空间
存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的。此时下式就无法给出经验误差和泛化误差的一个确定的上界：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

- 原因：以上不等式的上界被过度放大了，原因是在放大的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集

![](/assets/ee.png)

- 成长函数（Growth Function）：假设空间中有效模型的个数与样本容量的函数关系（将所有划分结果相同的模型看作是同一个模型），记做$$m_{H}(N)$$。成长函数取决于假设空间和样本分布。我们可以使用成长函数来代替不等式中的|H|来估计经验误差和泛化误差的差距。对于样本数为m的二分类问题，其有效模型数的一个上界为$$2^{m}$$，但是这个指数上界仍然过于宽泛了。下面我们的任务是将其缩减到一个多项式级别的函数。

![](/assets/34.png)

- **打碎（Shatter）**：对某个分布的样本集D和假设空间H，如果D的任何划分都能被H划分，则说该分布下的D能够被H打碎。下面第一个样本集可以被二维感知机shatter，第二个样本集不可以。

![](/assets/ss.png)
![](/assets/shatter.png)

- 突破点（Break Point）：对于假设空间H和整数k，如果任意分布的k个样本都不能被H打碎，则称k为H的突破点。突破点仅与假设空间有关，与样本**分布**和**划分**无关。很容易证明，如果k是H的突破点则任意比k大的整数也是H的突破点。

![](/assets/123.png)

####  VC维

- VC维：对于给定的假设空间H，它所能shatter的最大样本容量，称该假设空间的VC维。

VC维反应了函数集的学习能力，VC维越大学习能力越强，模型越复杂。遗憾的是目前尚没有通用的关于任意函数集VC维的计算理论，只能对一些特殊的函数集计算其VC维。例如N维空间中线性分类器的VC维是N+1。

VC维反映了假设空间H的强大程度，一个实践规律：VC维与假设参数w的自由变量数目大约相等。

$$
d_{vc} = free-parameters
$$

> H的VC维=H的最小突破点-1

- 上限函数（Bounding function）：B(N,k)，假设空间H的最小突破点为k时，H的成长函数的最大值。

$$
B(N,k)\leqslant B(N-1,k)+B(N-1,k-1)
$$

那么我们可以明显看出这是一个二项式系数，可得到通项公式：

$$
m_{H}(N)\leqslant B(N,k)\leqslant \sum_{i=0}^{k-1}C_{N}^{i}\leqslant N^{k-1}
$$

> 该式说明，如果假设空间H的VC维为$$d_{vc}$$（$$d_{vc}=k-1$$），假设空间成长函数的一个上限是$$N^{d_{vc}}$$，当样本容量足够大时，经验误差可以足够接近泛化误差：

$$
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 4m_{H}(2N)exp(-\frac{1}{8}\epsilon ^{2}N)\leqslant 4N^{d_{vc}}exp\left ( -\frac{1}{8}\epsilon ^{2} N \right )
$$

回到本文开始，为满足$$R(h) \approx R_{emp}(h)\approx 0$$，只需满足：

1. 训练样本容量N足够大
2. 假设空间的VC维有限

#### VC维和误差
将原不等式做一个改写：

![](/assets/vc_power1.png)

不等式右侧第一项表示经验误差，第二项表示模型复杂度。VC维数越大，代表模型的复杂度越大，经验误差越小，但经验误差和泛化误差相差就越远，总体上泛化误差先下降后上升。因此，应该选取合适的VC维数，用来决定应该使用多复杂的模型。

![](/assets/vcc.png)

#### VC维和样本数的关系
如果给定误差阈值$$\epsilon$$和精度$$\delta$$，那么$$d_{vc}$$和N的关系是什么呢?

![](/assets/bb.png)

因此我们可以看到在横线所在位置，N的增加和dvc几乎是一种线性的关系，经过工业界足够的现象表明，我们通常只要使得N≥10dvc就足够了。




























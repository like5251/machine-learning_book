> ## Why can Machines Learn?

## 一、形式化学习问题
**监督学习**：从带标签的训练集中识别出从特征到标签的某种模式，作为理想模式的近似。

**学习过程**：通过算法A，从假设空间中选取一个模型g，使得g在训练集D上的经验误差最小，将g作为理想模型f的近似。

![](/assets/tdml.png)

- 目标函数(Target Function)：$$y = f(x)$$，是未知的特征到标记的理想模式，$$y_{i}=f(x_{i})+\zeta $$，其中$$(x_{i},y_{i})$$是数据集中的样例，$$\zeta$$表示噪声
- 假设函数（Hypothesis Function）：$$y = h(x)$$是假设空间$$H=\{h:X \to Y  | h = h(x)\}$$中的某个模型
- 损失函数（Loss Function）：$$L=L(y,h(x))$$，计算样本预测值和真实值的误差

- 经验误差（Training error）：

$$
R_{emp}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i}))
$$

- 泛化误差（Generalization error）：

$$
R(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))]
$$

**学习可行的两个核心条件**：
1. 经验误差足够小：对应于训练阶段，对应于算法的偏差
2. 泛化误差足够接近经验误差：对应于测试阶段，对应于算法的方差

$$
R(h) \approx R_{emp}(h)\approx 0
$$

## 二、泛化界限
泛化误差的偏差-方差分解描述了算法偏差-算法方差-泛化误差随训练程度的变化情况，本质上是本文内容的定性版本。
- 算法偏差对应于经验误差，即算法对训练集的拟合程度，模型复杂度越高，算法偏差一般越小
- 算法方差对应于经验误差和泛化误差的差异，即数据扰动的影响，算法方差随样本容量增大指数级衰减，但会随模型复杂度而增大
- 训练程度对应于模型复杂度，即假设空间的大小，或说模型参数数量，或说算法的类型

$$
R\leqslant R_{emp}+\Omega (m,|H|,\epsilon )\\
E(f;D) = bias^{2}(x) + var(x) + \varepsilon ^{2}
$$

### 单一假设——无限样本集
由大数定理：

$$
\lim_{m \to\infty  }P[|E[X]-\overline{X}|>\epsilon ] = 0
$$

应用在泛化误差上：

$$
\lim_{m \to\infty  }P[|R(h)-R_{emp}(h)|>\epsilon ] = 0
$$

> 结论：对假设空间中任一假设，当训练集足够大时，泛化误差依概率等于经验误差

### 单一假设——有限样本集
由霍夫丁（hoeffding）不等式:

$$
P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )
$$

应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有：

$$
P[|R(h)-R_{emp}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )
$$

> 结论：对假设空间中任一假设，训练误差与泛化误差的差异随数据集大小成指数级衰减

### 多个假设
当有多个假设时会恶化算法的泛化性能（can get worse when involving choice），假设空间中存在某个假设使得训练误差和泛化误差的偏差大于$$\epsilon$$的概率为：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

> 结论：对于给定的假设空间，训练误差和泛化误差的差异随训练集增大而指数型衰减，但随假设空间增大而同比增大。


### 有穷假设空间
#### 样本容量
如果给定误差阈值$$\epsilon$$和精度$$\delta$$，我们需要多大的样本量才能保证训练误差和泛化误差差距不超过$$\epsilon$$的最小概率为$$1-\delta$$：

$$
m\geqslant \frac{1}{2\epsilon ^{2}}In\frac{2|H|}{\delta }
$$

上面不等式确定了m的一个下界，称为算法的样本复杂度。

#### 模型复杂度
问题|\|H\|很小|\|H\|很大
---|---|---
R ≈ Remp|满足|不满足
Remp≈0|不满足|满足

![](/assets/com.jpeg)

### 无穷假设空间
存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的。此时下式就无法给出经验误差和泛化误差的一个确定的上界：

$$
\begin{align*}
P[\underset{h\in H}{sup}|R(h)-R_{emp}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|R(h)-R_{emp}(h)|>\epsilon ]\\
&\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right )
\end{align*}
$$

- 原因：以上不等式的上界被过度放大了，原因是在放大的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集

![](/assets/ee.png)

- 成长函数（Growth Function）：假设空间中有效模型的个数与样本容量的函数关系（将所有划分结果相同的模型看作是同一个模型），记做$m_{H}(N)$。成长函数取决于假设空间和样本分布。我们可以使用成长函数来代替不等式中的|H|来估计经验误差和泛化误差的差距。对于样本数为m的二分类问题，其有效模型数的一个上界为$$2^{m}$$，但是这个指数上界仍然过于宽泛了。下面我们的任务是将其缩减到一个多项式级别的函数。

![](/assets/34.png)

- 打碎（Shatter）：给定一个容量为N的样本集D和一个假设空空间H，如果样本集的每一种分类情况都可以通过假设空间H得到，则称这个D可以被H打碎。能否shatter取决于假设空间和样本分布。下面第一个样本集可以被二维感知机shatter，第二个样本集不可以。

![](/assets/ss.png)
![](/assets/shatter.png)

- 突破点（Break Point）：对于假设空间H和整数k，如果任意分布的k个样本都不能被H打碎，则称k为H的突破点。很容易证明，如果k是H的突破点则任意比k大的整数也是H的突破点。

![](/assets/123.png)


####  VC维

生长函数（growth function）：假设空间的大小关于数据集大小的函数。对于二分类问题，生长函数的上界为：

$$
\bigtriangleup _{H}(m)\leqslant 2^{m}
$$

指数上界仍然太大，我们希望能找到一个多项式级别的生长函数上界。

突破点（break point）:空间H不能打碎（shatter）的最小点数。如感知机最小不能打碎4个点，突破点为4。

上界函数（Bounding Function）：长度为N的向量中满足任意k个分量都不能被打碎（全部出现）的向量个数，定义为B(N,k)

































